# Decoder-Only LLM LoRA Fine-Tuning with Compressed Smashed Data

This project explores fine-tuning decoder-only large language models using LoRA techniques while incorporating compressed smashed-data representations.  
The goal is to enable efficient model partitioning and reduce communication overhead during distributed or privacy-preserving training setups.

## Overview

- **Model Type:** Decoder-only LLM  
- **Method:** LoRA (Low-Rank Adaptation) fine-tuning  
- **Data Strategy:** Compressed smashed-data inputs to reduce transfer size  
- **Objective:** Improve efficiency when partitioning models across devices or nodes

## Status

**Work in progressâ€¦**
